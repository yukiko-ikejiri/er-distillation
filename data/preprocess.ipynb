{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This notebook includes all the code to preprocess datasets for the experiments in the paper. There will be two main parts: the first half is for row pairs preparation, while the second half is for attribute pairs preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Row Pairs Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training & Validation Set\n",
    "\n",
    "The magellan datasets and wdc dataset need different preparation steps. We will first prepare the magellan datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Magellan Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "magellan_dirs = {\n",
    "    'amgo': 'raw/amazon_google',\n",
    "    'beer': 'raw/beer', 'dbac': 'raw/dblp_acm',\n",
    "    'dbgo': 'raw/dblp_scholar', 'foza': 'raw/fodors_zagat',\n",
    "    'itam': 'raw/itunes_amazon', 'waam': 'raw/walmart_amazon',\n",
    "}\n",
    "\n",
    "magellan_rename_columns = {\n",
    "    'amgo': ['id', 'name', 'manufacturer', 'price'],\n",
    "    'beer': ['id', 'name', 'factory', 'style', 'ABV'], 'dbac': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'dbgo': ['id', 'title', 'authors', 'venue', 'year'], 'foza': ['id', 'name', 'address', 'city', 'phone', 'type', 'class'],\n",
    "    'itam': ['id', 'name', 'artist', 'album', 'genre', 'price', 'copyright', 'time', 'released'],\n",
    "    'waam': ['id', 'name', 'category', 'brand', 'modelno', 'price'],\n",
    "}\n",
    "\n",
    "magellan_drop_columns = {\n",
    "    'amgo': ['manufacturer'], 'beer': [], 'dbac': [], 'dbgo': [], 'foza': [], 'itam': [],\n",
    "    'waam': ['category', 'brand'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reasoning(text, remove_answer=False):\n",
    "    if pd.isna(text):\n",
    "        return (None, \"\")\n",
    "    \n",
    "    label = None\n",
    "    if \"Answer: Yes.\" in text:\n",
    "        label = 1\n",
    "    elif \"Answer: No.\" in text:\n",
    "        label = 0\n",
    "        \n",
    "    if remove_answer:\n",
    "        processed_text = text.replace(\"Answer: Yes.\", \"\").replace(\"Answer: No.\", \"\")\n",
    "    else:\n",
    "        processed_text = text\n",
    "            \n",
    "    return label, processed_text.strip()\n",
    "\n",
    "def filter_valid_reasoning(df):\n",
    "    valid_mask = (\n",
    "        df['reasoning_text'].notna() & \n",
    "        (df['reasoning_text'] != '') & \n",
    "        (~df['reasoning_text'].str.contains('Error code: 400', na=False)) \n",
    "    )\n",
    "    filtered_df = df[valid_mask].reset_index(drop=True)\n",
    "    removed_count = len(df) - len(filtered_df)\n",
    "    print(f\"Filtered {removed_count} samples with empty reasoning_text\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def add_reasoning_to_df(df, reasoning_dict, remove_answer=False):\n",
    "    reasoning_texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        key = (row['ltable_id'], row['rtable_id'])\n",
    "        reasoning_text = reasoning_dict.get(key, \"\")\n",
    "        reasoning_texts.append(reasoning_text)\n",
    "    df['reasoning_text'] = reasoning_texts\n",
    "    \n",
    "    df = filter_valid_reasoning(df)\n",
    "\n",
    "    results = df['reasoning_text'].apply(analyze_reasoning)\n",
    "    df['answer_label'] = [result[0] for result in results]\n",
    "    df['reasoning_text'] = [result[1] for result in results]\n",
    "\n",
    "    yes_count = (df['answer_label'] == 1).sum()\n",
    "    no_count = (df['answer_label'] == 0).sum()\n",
    "    unclassified = df['answer_label'].isna().sum()\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"'Yes' (1) count: {yes_count}\")\n",
    "    print(f\"'No' (0) count: {no_count}\")\n",
    "    print(f\"Unclassified data count: {unclassified}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_reasoning_data(dataset_name, split):\n",
    "    reasoning_file = f'reasoning/{dataset_name}/{split}_results.jsonl'\n",
    "    if not os.path.exists(reasoning_file):\n",
    "        print(f\"Warning: Reasoning file {reasoning_file} not found\")\n",
    "        return {}\n",
    "    \n",
    "    reasoning_dict = {}\n",
    "    with open(reasoning_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            custom_id = data['custom_id']\n",
    "            ltable_id, rtable_id = custom_id.split('_')\n",
    "            reasoning_dict[(int(ltable_id), int(rtable_id))] = data['response']\n",
    "    \n",
    "    return reasoning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_with_id(tableA, tableB, id_pairs):\n",
    "    left_merged = pd.merge(tableA, id_pairs, left_on='id', right_on='ltable_id')\n",
    "    left_right_merged = pd.merge(left_merged, tableB, left_on='rtable_id', right_on='id', suffixes=('_l', '_r'))\n",
    "    left_right_merged.drop(columns=['id_l', 'id_r'], inplace=True)\n",
    "    return left_right_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_magellan_row_pairs(dirs: dict, rename_columns: dict, drop_columns: dict, remove_answer: bool):\n",
    "    for d_name in dirs:\n",
    "        tableA = pd.read_csv(os.path.join(dirs[d_name], 'tableA.csv'))\n",
    "        tableB = pd.read_csv(os.path.join(dirs[d_name], 'tableB.csv'))\n",
    "        tableA.columns = rename_columns[d_name]\n",
    "        tableB.columns = rename_columns[d_name]\n",
    "        tableA.drop(columns=drop_columns[d_name], inplace=True)\n",
    "        tableB.drop(columns=drop_columns[d_name], inplace=True)\n",
    "\n",
    "        train_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'train.csv'))\n",
    "        valid_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'valid.csv'))\n",
    "        test_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'test.csv'))\n",
    "        train_df = merge_with_id(tableA, tableB, train_id_pairs)\n",
    "        valid_df = merge_with_id(tableA, tableB, valid_id_pairs)\n",
    "        test_df = merge_with_id(tableA, tableB, test_id_pairs)\n",
    "        \n",
    "        train_reasoning = load_reasoning_data(d_name, 'train')\n",
    "        valid_reasoning = load_reasoning_data(d_name, 'valid')\n",
    "        test_reasoning = load_reasoning_data(d_name, 'test')\n",
    "        train_df = add_reasoning_to_df(train_df, train_reasoning, remove_answer)\n",
    "        valid_df = add_reasoning_to_df(valid_df, valid_reasoning, remove_answer)\n",
    "        test_df = add_reasoning_to_df(test_df, test_reasoning, remove_answer)\n",
    "        train_df.drop(columns=['ltable_id', 'rtable_id'], inplace=True)\n",
    "        valid_df.drop(columns=['ltable_id', 'rtable_id'], inplace=True)\n",
    "        test_df.drop(columns=['ltable_id', 'rtable_id'], inplace=True)\n",
    "\n",
    "        if not os.path.exists(f'prepared/{d_name}'):\n",
    "            os.makedirs(f'prepared/{d_name}')\n",
    "        train_df.to_csv(f'prepared/{d_name}/train.csv', index=False)\n",
    "        valid_df.to_csv(f'prepared/{d_name}/valid.csv', index=False)\n",
    "        test_df.to_csv(f'prepared/{d_name}/test.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prepare_magellan_row_pairs(\n",
    "    magellan_dirs, \n",
    "    magellan_rename_columns, \n",
    "    magellan_drop_columns,\n",
    "    remove_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# AutoML Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_automl_predictions():\n",
    "    dataset_names = ['amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'waam']\n",
    "    for name in dataset_names:\n",
    "        train_df = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_df = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "\n",
    "        feature_columns = [col for col in train_df.columns if col not in ['reasoning_text', 'answer_label', 'label']]\n",
    "        train_automl = train_df[feature_columns + ['label']].copy()\n",
    "        valid_automl = valid_df[feature_columns + ['label']].copy()\n",
    "\n",
    "        predictor = TabularPredictor(label='label').fit(train_data=train_automl, tuning_data=valid_automl, verbosity=-1)\n",
    "        train_preds = predictor.predict(train_automl)\n",
    "        train_preds_proba = predictor.predict_proba(train_automl)\n",
    "        valid_preds = predictor.predict(valid_automl)\n",
    "        valid_preds_proba = predictor.predict_proba(valid_automl)\n",
    "        train_preds_df = pd.DataFrame({'prediction': train_preds, 'proba_0': train_preds_proba[0], 'proba_1': train_preds_proba[1]})\n",
    "        valid_preds_df = pd.DataFrame({'prediction': valid_preds, 'proba_0': valid_preds_proba[0], 'proba_1': valid_preds_proba[1]})\n",
    "\n",
    "        if not os.path.exists(f'automl/{name}'):\n",
    "            os.makedirs(f'automl/{name}')\n",
    "        train_preds_df.to_csv(f'automl/{name}/train_preds.csv', index=False)\n",
    "        valid_preds_df.to_csv(f'automl/{name}/valid_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250408_183719\"\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250408_184026\"\n"
     ]
    }
   ],
   "source": [
    "prepare_automl_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
